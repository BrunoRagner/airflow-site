:py:mod:`airflow.providers.apache.hdfs.sensors.hdfs`
====================================================

.. py:module:: airflow.providers.apache.hdfs.sensors.hdfs


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   airflow.providers.apache.hdfs.sensors.hdfs.HdfsSensor
   airflow.providers.apache.hdfs.sensors.hdfs.HdfsRegexSensor
   airflow.providers.apache.hdfs.sensors.hdfs.HdfsFolderSensor




Attributes
~~~~~~~~~~

.. autoapisummary::

   airflow.providers.apache.hdfs.sensors.hdfs.log


.. py:data:: log
   

   

.. py:class:: HdfsSensor(*, filepath: str, hdfs_conn_id: str = 'hdfs_default', ignored_ext: Optional[List[str]] = None, ignore_copying: bool = True, file_size: Optional[int] = None, hook: Type[airflow.providers.apache.hdfs.hooks.hdfs.HDFSHook] = HDFSHook, **kwargs: Any)

   Bases: :py:obj:`airflow.sensors.base.BaseSensorOperator`

   Waits for a file or folder to land in HDFS

   :param filepath: The route to a stored file.
   :type filepath: str
   :param hdfs_conn_id: The Airflow connection used for HDFS credentials.
   :type hdfs_conn_id: str
   :param ignored_ext: This is the list of ignored extensions.
   :type ignored_ext: Optional[List[str]]
   :param ignore_copying: Shall we ignore?
   :type ignore_copying: Optional[bool]
   :param file_size: This is the size of the file.
   :type file_size: Optional[int]

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:HdfsSensor`

   .. py:attribute:: template_fields
      :annotation: :Sequence[str] = ['filepath']

      

   .. py:attribute:: ui_color
      

      

   .. py:method:: filter_for_filesize(result: List[Dict[Any, Any]], size: Optional[int] = None) -> List[Dict[Any, Any]]
      :staticmethod:

      Will test the filepath result and test if its size is at least self.filesize

      :param result: a list of dicts returned by Snakebite ls
      :param size: the file size in MB a file should be at least to trigger True
      :return: (bool) depending on the matching criteria


   .. py:method:: filter_for_ignored_ext(result: List[Dict[Any, Any]], ignored_ext: List[str], ignore_copying: bool) -> List[Dict[Any, Any]]
      :staticmethod:

      Will filter if instructed to do so the result to remove matching criteria

      :param result: list of dicts returned by Snakebite ls
      :type result: list[dict]
      :param ignored_ext: list of ignored extensions
      :type ignored_ext: list
      :param ignore_copying: shall we ignore ?
      :type ignore_copying: bool
      :return: list of dicts which were not removed
      :rtype: list[dict]


   .. py:method:: poke(self, context: airflow.utils.context.Context) -> bool

      Get a snakebite client connection and check for file.



.. py:class:: HdfsRegexSensor(regex: Pattern[str], *args: Any, **kwargs: Any)

   Bases: :py:obj:`HdfsSensor`

   Waits for matching files by matching on regex

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:HdfsRegexSensor`

   .. py:method:: poke(self, context: airflow.utils.context.Context) -> bool

      Poke matching files in a directory with self.regex

      :return: Bool depending on the search criteria



.. py:class:: HdfsFolderSensor(be_empty: bool = False, *args: Any, **kwargs: Any)

   Bases: :py:obj:`HdfsSensor`

   Waits for a non-empty directory

   .. seealso::
       For more information on how to use this operator, take a look at the guide:
       :ref:`howto/operator:HdfsFolderSensor`

   .. py:method:: poke(self, context: airflow.utils.context.Context) -> bool

      Poke for a non empty directory

      :return: Bool depending on the search criteria



